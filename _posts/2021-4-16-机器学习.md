---
title: 机器学习
type: post
tag: 机器学习
---

# 西瓜书笔记

## 1. 基本术语

* 分类（classification）预测值为离散值的问题(好瓜 坏瓜)：。
* 回归（regression） 预测值为连续值的问题 西瓜成熟度0.95， 0.26：。
* 聚类（clustering）将训练集中的西瓜分成若干组，每组称为一个簇，例如本地瓜 深色瓜。。。
* 泛化能力（generalization）学得模型适用于新样本的能力
* 训练数据有标记信息的学习任务为：监督学习（supervised learning），容易知道上面所描述的分类和回归都是监督学习的范畴。
* 训练数据没有标记信息的学习任务为：无监督学习（unsupervised learning），常见的有聚类和关联规则。
* 归纳偏好（inductive bias） 机器学习算法在学习过程中对某种类型假设的偏好
* 学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：过拟合（overfitting）。
* 学习能太差，训练样本的一般性质尚未学好，称为：欠拟合（underfitting）。

## 2. 常用方法

### 2.1 划分训练集与测试集的方法  

   如果只有一个包含m个样例的数据集D，既要训练，又要测试，就需要对D进行适当的处理

#### 2.1.1 留出法  

    直接将D划分为两个互斥的集合，其中一个作为训练集S，另一个作为测试集T 即
    D = S ∪ T ，S ∩ T = 0

#### 2.1.2 交叉验证法

    先将D划分为k个大小相似的互斥子集，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就获得k组训练/测试集

#### 2.1.3 自助法

线性回归（linear regression）
===============

## 连续化

   将离散值转化为连续值，如

   * 高，矮--> {1.0,0.0}
   * 高，中，矮-->{1.0,0.5,0.0}  
     属性间不存在序关系，通常转化为k维向量，如
   * 西瓜（0，0，1） 南瓜（0，1，0） 黄瓜（1，0，0）

# 决策树的建立

<img src="H:\gitee\notes\2021\April\imgs\jueceshu.jpg"/>

要建立如上图所示的决策树，问题在于**为什么把纹理而非根蒂或者触感放在第一层？**
决策树中，判断哪个特征在哪一层常用两种算法-->**ID3和ID4.5**  
集合D中的数据如下图所示  
<img src="H:\gitee\notes\2021\April\imgs\data.png"/>

## ID３

在ID3中使用**信息增益**来判断特征在哪一层

首先阐述三个概念**信息熵、条件熵、信息增益**

* 1.信息熵Ent(D)

假设集合D有两种类别，好瓜和坏瓜，第k类样本所占的比例为pk,则信息熵的定义为

<img src="H:\gitee\notes\2021\April\imgs\ent.png"/>

其中|y|代表集合中的类别数量  
如果D中好瓜有8个，坏瓜有9个，就能算出集合D的信息熵如下  
<img src="E:\gitfile\notes\April\imgs\ent.jpg"/>

* 2.条件熵
  离散属性 a 有 V 个可能的取值 {a1,a2,…,aV}；样本集合中，属性 a 上取值为 av 的样本集合，记为 Dv。条件熵的定义为
* 3.信息增益  
  信息增益的定义为  
  <img src="H:\gitee\notes\2021\April\imgs\gain.jpg"/>

信息增益表示得知属性 a 的信息而使得样本集合不确定度减少的程度  
根据ID３算法，选择信息增益最大的特征来构建决策树  
接下来根据上述规则计算当前**属性集合{色泽，根蒂，敲声，纹理，脐部，触感}中每个属性的信息增益**  

色泽有3个可能的取值：{青绿，乌黑，浅白}  
D1(色泽=青绿) = {1, 4, 6, 10, 13, 17}，正例  3/6，反例 3/6

D2(色泽=乌黑) = {2, 3, 7, 8, 9, 15}，正例  4/6，反例 2/6

D3(色泽=浅白) = {5, 11, 12, 14, 16}，正例  1/5，反例 4/5
**3个分支结点的信息熵**
<img src="E:\gitfile\notes\April\imgs\ent1.jpg"/>  
**由此计算特征色泽的信息增益是：**  
<img src="E:\gitfile\notes\April\imgs\ent2.jpg"/>  
**同理，我们可以求出其它属性的信息增益，分别如下：**
<img src="https://pic3.zhimg.com/v2-31a579d93bb2ea4b8936d4c416fdb03e_b.png"/>  
**于是找到了信息增益最大的属性为纹理：如下**  
<img src="E:\gitfile\notes\April\imgs\root.jpg"/>  
根据这三个子节点，把集合D划分为3个子集D1{1，2，3，4，5，6，8，10，15}D2{7，9，13，14，17}D3{11，12，16}  
在这三个子集上再递归求特征，以D1为例，D1(纹理=清晰) = {1, 2, 3, 4, 5, 6, 8, 10, 15}，第一个分支结点可用属性集合{色泽、根蒂、敲声、脐部、触感}，基于 D1各属性的信息增益，分别求的如下：  
<img src="E:\gitfile\notes\April\imgs\allgains.jpg"/>  
于是我们可以选择特征属性为根蒂，脐部，触感三个特征属性中任选一个（因为他们三个相等并最大），其它俩个子结点同理，然后得到新一层的结点，再递归的由信息增益进行构建树即可  
**我们最终的决策树如下：**  
<img src="H:\gitee\notes\2021\April\imgs\ans.jpg"/>

# 1 术语

1. **欧氏距离**：二维空间两点之间距离，ρ=[(x2-x1)^2 + (y2-y1)^2]^1/2 

2. **曼哈顿距离**：两个点在标准坐标系上的绝对轴距总和。d= |x2-x1| + |y2-y1|

   **下图中，红色表示曼哈顿距离，绿色表示欧氏距离**

   <img src="H:\gitfile\modiman.github.io\docs\_posts\imgs\juli.png">

   3. **归一化**：把数据映射到[0,1]区间内

      例如 最小-最大标准化策略计算公式

      <img src="H:\gitfile\modiman.github.io\docs\_posts\imgs\guiyihua.png">

​         min(a_i)和max⁡(a_i )分别表示第i个属性值a_i在所有球队中的最小值和最大值。

# 2. k-均值

| 队伍  | X_1  | X_2  | X_3  | X_4  | X_5  | X_6  | X_7  | X_8  |
| ----- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 赛事1 | 50   | 28   | 17   | 25   | 28   | 50   | 50   | 50   |
| 赛事2 | 50   | 9    | 15   | 40   | 40   | 50   | 40   | 40   |
| 赛事3 | 9    | 4    | 3    | 5    | 2    | 1    | 9    | 9    |
| 队伍  | X_9  | X_10 | X_11 | X_12 | X_13 | X_14 | X_15 |      |
| 赛事1 | 40   | 50   | 50   | 50   | 40   | 40   | 50   |      |
| 赛事2 | 40   | 50   | 50   | 50   | 40   | 32   | 50   |      |
| 赛事3 | 5    | 9    | 5    | 9    | 9    | 17   | 9    |      |

**问题：**要将上述15支 球队的水平分为3个层次

**过程：**

## 2.1.首先将分数归一化处理得下表：

| 队伍  | X_1  | X_2  | X_3  | X_4  | X_5   | X_6  | X_7  | X_8  |
| ----- | ---- | ---- | ---- | ---- | ----- | ---- | ---- | ---- |
| 赛事1 | 1    | 0.3  | 0    | 0.24 | 0.3   | 1    | 1    | 1    |
| 赛事2 | 1    | 0    | 0.15 | 0.76 | 0.76  | 1    | 0.76 | 0.76 |
| 赛事3 | 0.5  | 0.19 | 0.13 | 0.25 | 0．06 | 0    | 0.5  | 0.5  |
| 队伍  | X_9  | X_10 | X_11 | X_12 | X_13  | X_14 | X_15 |      |
| 赛事1 | 0.7  | 1    | 1    | 1    | 0.7   | 0.7  | 1    |      |
| 赛事2 | 0.76 | 1    | 1    | 1    | 0.76  | 0.68 | 1    |      |
| 赛事3 | 0.25 | 0.5  | 0.25 | 0.5  | 0.5   | 1    | 0.5  |      |

## 2.2 随机选取中心

**由于需将球队分为3个层次水平，故取聚类的簇数k=3。通过随机采样选择编号为2、11、14的三支队伍所对应数据点作为初始聚类中心，即三个簇的聚类中心分别为：μ_1=(0.3,0,0.19),μ_2=(0.7,0,76,0.5),μ_3=(1,1,0.5)**

## 2.3 计算每个数据点到聚类中心的欧氏距离

计算结果如表所示。

| 队伍 | X_1    | X_2    | X_3    | X_4    | X_5    | X_6    | X_7    | X_8    |
| ---- | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ |
| μ_1  | 1.2594 | 0      | 0.3407 | 0.7647 | 0.7710 | 1.2354 | 1.0787 | 1.0787 |
| μ_2  | 0      | 0.9131 | 0.9995 | 0.5235 | 0.5946 | 0.6306 | 0.3000 | 0.3000 |
| μ_3  | 0.3407 | 1.2594 | 1.3636 | 0.8353 | 0.8609 | 0.5000 | 0.2400 | 0.2400 |
| 队伍 | X_9    | X_10   | X_11   | X_12   | X_13   | X_14   | X_15   |        |
| μ_1  | 0.8609 | 1.2594 | 1.2221 | 1.2594 | 0.9131 | 1.1307 | 1.2594 |        |
| μ_2  | 0.2500 | 0.3842 | 0.4584 | 0.3842 | 0      | 0.5064 | 0.3842 |        |
| μ_3  | 0.4584 | 0      | 0.2500 | 0      | 0.3842 | 0.6651 | 0      |        |

## 2.4 分配

对于每个数据点，分别计算它与三个聚类中心的距离，取最小的值作为它的分类

得到第一次结果

* C_1={X_2,X_3}；
* C_2={X_4,X_5,X_9,X_13,X_14}；
* C_3={X_1,X_6,X_7,X_8,X_10,X_11,X_12,X_15}

## 2.5 调整聚类中心

根据上述第一次聚类结果，对聚类中心做调整。对于C_1，有：

 <img src="H:\gitfile\modiman.github.io\docs\_posts\imgs\julei.png">

同理可将第二个簇C_2和第三个簇C_3的聚类中心进行调整，分别得到

μ_2^′=(0.528,0.744,0.412),μ_3^′=(1,0.94,0.40625)

## 2.6再次计算

**计算各数据点与更新后的聚类中心的距离，得到如表所示计算结果**

| 队伍  | X_1    | X_2    | X_3    | X_4    | X_5    | X_6    | X_7    | X_8    |
| ----- | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ |
| μ_1^′ | 1.3014 | 0.1704 | 0.1704 | 0.6967 | 0.7083 | 1.2664 | 1.1434 | 1.1434 |
| μ_2^′ | 0.5441 | 0.8092 | 0.8443 | 0.3308 | 0.4197 | 0.6768 | 0.4804 | 0.4804 |
| μ_3^′ | 0.1113 | 1.1918 | 1.3040 | 0.7965 | 0.8014 | 0.4107 | 0.2030 | 0.2030 |
| 队伍  | X_9    | X_10   | X_11   | X_12   | X_13   | X_14   | X_15   |        |
| μ_1^′ | 0.8831 | 1.3014 | 1.2595 | 1.3014 | 0.9420 | 1.1722 | 1.3014 |        |
| μ_2^′ | 0.2368 | 0.5441 | 0.5609 | 0.5441 | 0.1939 | 0.6160 | 0.5441 |        |
| μ_3^′ | 0.3832 | 0.1113 | 0.1674 | 0.1113 | 0.3622 | 0.7142 | 0.1113 |        |

根据表可得到第二次聚类结果如下：C_1={X_2,X_3}；C_2= {X_4,X_5,X_9,X_13,X_14}；C_3={X_1,X_6,X_7,X_8,X_10,X_11,X_12,X_15}

聚类结果并未发生变化，故聚类中心收敛，停止迭代。



## 2.7 结论

由上述聚类结果可知，X_2 、X_3两支球队的整体水平比较相近，X_4,X_5,X_9,X_13,X_14的整体水平比较相近，其余球队的整体水平比较相近。

# 3.softmax函数

参考

* 百度百科
* 知乎文章：https://www.zhihu.com/search?q=softmax%E5%87%BD%E6%95%B0&utm_content=search_suggestion&type=content

在数学，尤其是概率论和相关领域中，**归一化指数函数，或称[Softmax函数]**，是逻辑函数的一种推广。它能将一个含任意实数的K维向量z“压缩”到另一个K维实向量σ(z)中，使得每一个元素的范围都在(0,1)之间，并且所有元素的和为1。该函数多于**多分类问题**中。

```
import math

z = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]

z_exp = [math.exp(i) for i in z]

print(z_exp)  # Result: [2.72, 7.39, 20.09, 54.6, 2.72, 7.39, 20.09]

sum_z_exp = sum(z_exp)
print(sum_z_exp)  # Result: 114.98
# Result: [0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]

softmax = [round(i / sum_z_exp, 3) for i in z_exp] #结果保留三位小数
print(softmax)
```

## 3.1 softmax 相对于 hardmax

softmax从字面上来说，可以分成**soft**和**max**两个部分。max故名思议就是最大值的意思。Softmax的核心在于soft，而soft有软的含义，与之相对的是**hard**硬。**很多场景中需要我们找出数组所有元素中值最大的元素，实质上都是求的hardmax**。下面使用Numpy模块以及TensorFlow深度学习框架实现hardmax。

```Py
import numpy as np
a = np.array([1, 2, 3, 4, 5]) # 创建ndarray数组
a_max = np.max(a)
print(a_max) # 5
```

通过上面的例子可以看出hardmax最大的特点就是**只选出其中一个最大的值，即非黑即白**。但是往往在实际中这种方式是不合情理的，比如对于文本分类来说，一篇文章或多或少包含着各种主题信息，我们更期望得到**文章对于每个可能的文本类别的概率值（置信度）**，可以简单理解成属于对应类别的可信度。所以此时用到了soft的概念，Softmax的含义就在于**不再唯一的确定某一个最大值，而是为每个输出分类的结果都赋予一个概率值**，表示属于每个类别的可能性。



## 3.2优点

指数函数曲线呈现递增趋势，最重要的是斜率逐渐增大，也就是说在x轴上一个很小的变化，可以导致y轴上很大的变化。这种函数曲线能够将输出的数值拉开距离。假设拥有三个输出节点的输出值为 ![[公式]](https://www.zhihu.com/equation?tex=z_%7B1%7D%2Cz_%7B2%7D%2Cz_%7B3%7D) 为[2, 3, 5]。首先尝试不使用指数函数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7Bz_%7Bi%7D%7D%7B%5Csum_%7Bc+%3D+1%7D%5E%7B3%7D%7Bz_%7Bc%7D%7D%7D) ，接下来使用指数函数的Softmax函数计算。

```Pyt
import tensorflow as tf

print(tf.__version__) # 2.0.0
a = tf.constant([2, 3, 5], dtype = tf.float32)

b1 = a / tf.reduce_sum(a) # 不使用指数
print(b1) # tf.Tensor([0.2 0.3 0.5], shape=(3,), dtype=float32)

b2 = tf.nn.softmax(a) # 使用指数的Softmax
print(b2) # tf.Tensor([0.04201007 0.11419519 0.8437947 ], shape=(3,), dtype=float32)
```

两种计算方式的输出结果分别是：

- tf.Tensor([0.2 0.3 0.5], shape=(3,), dtype=float32)
- tf.Tensor([0.04201007 0.11419519 0.8437947 ], shape=(3,), dtype=float32)

## 3.3 缺点

指数函数的曲线斜率逐渐增大虽然能够将输出值拉开距离，但是也带来了缺点，当 ![[公式]](https://www.zhihu.com/equation?tex=z_%7Bi%7D) 值非常大的话，计算得到的数值也会变的非常大，数值可能会溢出。

```Python
import numpy as np

scores = np.array([123, 456, 789])
softmax = np.exp(scores) / np.sum(np.exp(scores))
print(softmax) # [ 0.  0. nan]

```

当然针对数值溢出有其对应的优化方法，将每一个输出值减去输出值中最大的值。

```
import numpy as np

scores = np.array([123, 456, 789])
scores -= np.max(scores)
p = np.exp(scores) / np.sum(np.exp(scores))

print(p) # [5.75274406e-290 2.39848787e-145 1.00000000e+000]

```



得到的距离如下

| team | μ1   | μ2   | μ3   |
| ---- | ---- | ---- | ---- |
| x1   | 1.15 | 0.66 | 0.72 |
| x2   | 1.47 | 0.8  | 0.95 |
| x3   | 1.44 | 1.04 | 1.22 |
| x4   | 1.07 | 0.84 | 1.02 |
| x5   | 1.27 | 0.89 | 0.94 |
| x6   | 1.54 | 0.91 | 0.65 |
| x7   | 1.16 | 0.43 | 0.53 |
| x8   | 1.16 | 0.43 | 0.53 |
| x9   | 1.19 | 0.58 | 0.61 |
| x10  | 1.15 | 0.66 | 0.72 |
| x11  | 1.33 | 0.75 | 0.64 |
| x12  | 1.15 | 0.66 | 0.72 |
| x13  | 0.97 | 0.45 | 0.7  |
| x14  | 0.71 | 0.43 | 0.99 |
| x15  | 1.15 | 0.66 | 0.72 |