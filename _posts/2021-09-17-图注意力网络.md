---
title:图注意力网络
type: post
tag: 深度学习
---

# GRAPH ATTENTION NETWORKS 

**图注意力网络的目的是聚合邻居节点的信息来提高自身的表示能力**



![在这里插入图片描述](https://img-blog.csdnimg.cn/95d342bdbba74c5d8e8323408fd2430f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbW9kaWdlX2ht,size_20,color_FFFFFF,t_70,g_se,x_16)

## 2.GAT架构

### 2.1 GRAPH ATTENTIONAL LAYER

从一个单层图注意力层开始介绍

**输入**：节点特征：![在这里插入图片描述](https://img-blog.csdnimg.cn/77473219c5d44c54b97d33441a87082b.png#pic_center)
                （ N 是节点数量 ，F是节点特征数量）

**输出**是一个新的节点特征

 ![h<sup>'</sup>={h1',h2',h3}](https://img-blog.csdnimg.cn/dba2f7c486944d45a502749d7961ab7d.png)
为了把输入的特征变为输出特征，引入一个权重矩阵**W**（F'行F列）
![在这里插入图片描述](https://img-blog.csdnimg.cn/750526e014a247cd9c0d0cacc2cdfbe5.png)

接着对每个节点实施self-attention]
![在这里插入图片描述](https://img-blog.csdnimg.cn/f1a72c3d97594f1bb73ae53d2e3236e5.png)
计算注意力系数
![注意力系数](https://img-blog.csdnimg.cn/a28f436ce5154c3e9f99c0ea60dbd739.png)
这一系数代表节点j的特征对节点i的重要程度（节点j是节点i的相邻节点）
为了方便比较不同节点的系数，使用softmax对所有节点的系数进行正则化
![在这里插入图片描述](https://img-blog.csdnimg.cn/0dcd576d08234fb392c30373ecd114b6.png)
本实验中，注意力机制a是一个单层的前馈神经网络，由权重向量**a**参数化（2F'行1列）
![在这里插入图片描述](https://img-blog.csdnimg.cn/18e2d4f63cc04080beef5fcf89945972.png)
然后使用  LeakyReLU函数正则化（<0部分斜率为0.2）

![在这里插入图片描述](https://img-blog.csdnimg.cn/43f885b114f442ae93b6c915a035e34c.png)
|| 为拼接运算，例如：（1，2）<sup>T</sup>||（1，2）<sup>T</sup>=(1,2,1,2)<sup>T</sup>

最后得出
![在这里插入图片描述](https://img-blog.csdnimg.cn/99a390ccca7a448cba7d084a6050b1fb.png)
为了使self-attention的学习过程更加稳定，使用多头注意力机制，![在这里插入图片描述](https://img-blog.csdnimg.cn/99cbaffff1c7456fbc8286644f2ac227.png)

|| 为拼接运算，
α<sup>k</sup><sub>ij</sub>是由α<sup>k</sup>计算得来的正则化注意力系数，
**W**<sub>k</sub>是对应输入线性变换的权重矩阵
最终的输出**h'**包含K*F‘个向量
    如果我们在网络的最终（预测）层上执行多头注意，连接不再是明智的，相反，我们采用平均化，并延迟应用最终非线性（通常是分类问题的softmax或logistic sigmoid），直到：![在这里插入图片描述](https://img-blog.csdnimg.cn/f8b0f620811345d190026418811777c7.png)

## GRAPH ATTENTION NETWORKS 图注意力网络
