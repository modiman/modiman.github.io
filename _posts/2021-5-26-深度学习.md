---
title: 深度学习
type: post
tag: 深度学习
---

[TOC]

 ### 1.1神经元



 <img src ="H:\gitfile\modiman.github.io\docs\_posts\imgs\shenjingyuan.png">

​                                             <font color = "red">**神经元**</font>

**对单个神经元而言，其输入信息来自多个不同的神经元**

<img src="H:\gitfile\modiman.github.io\docs\_posts\imgs\ppt3.png"/>

​         <font color="blue" size="5px" family="songti">**激活函数σ**的作用是对神经元的输出加以限制使得其有界，通常将输出范围限制在[0,1]或[-1,1]。</font>

###  1.2  感知机

**感知机模型是一种只有一层神经元参与数据处理的人工神经网络模型**

感知机模型通过输入层接受输入信息X=〖(x_1,x_2,⋯,x_m)〗^T，感知机模型的输出为：

<img src="H:\gitfile\modiman.github.io\docs\_posts\imgs\ppt4.png" style="position:left"/>

（sgn:符号函数，返回参数的正负）

可将感知机输入表示为向量形式，输入向量为X=(1,x1,x2,......xm)<sup>T</sup>,连接权重为W=（b,w1,w2,........wm）<sup>T</sup>

由此可将感知机输出表示为如下形式：

<img src="H:\gitfile\modiman.github.io\docs\_posts\imgs\ganzhiji.png"/>

使用感知机模型**可解决**二维平面中线性可分的二分类问题，但感知机模型**难以解决**线性不可分问题和多分类任务

# 2. 深度学习

## 2.1 神经网络



## 2.2 卷积神经网络（CNN）

### 2.2.1 卷积

卷积是一类对矩阵的线性运算方式，

### 2.2.2 卷积层

包含卷积操作的网络层被称为卷积层，其用来对输入图片进行特征提取，输出相应图片的特征图。

### 2.2.3 卷积核

**矩阵A是样本数据矩阵**

**矩阵B称之为卷积核**



<img src ="H:\gitfile\modiman.github.io\docs\_posts\imgs\juanjihe.png"/>

​                               矩阵A                                   矩阵B

<img src ="H:\gitfile\modiman.github.io\docs\_posts\imgs\juanjishiyi.png"/>

​                        卷积操作示意                           矩阵C

**卷积操作得到特征矩阵C**

### 2.2.4 步长

卷积核每次移动的长度

### 2.2.5 填充(padding)

在实际操作中，步长越大，卷积操作所得到的特征矩阵越小，并且数据矩阵边界上的元素对特征矩阵的贡献较小，即丢失部分边界信息，为解决这些问题,通常会为原始数据填补上一圈或几圈元素

### 2.2.6 池化（polling）

池化层亦称Pooling层，其操作是池化，即**下采样**，主要作用是通过去除输入的特征图中不重要的信息，使特征图变小，进行特征压缩，进一步减少参数量，且同时提取其中的有效信息。

<img src ="H:\gitfile\modiman.github.io\docs\_posts\imgs\yuanshijuzhen.png"/>

#### 2.2.6.1 最大池化

 <img src ="H:\gitfile\modiman.github.io\docs\_posts\imgs\zuida.png"/>

#### 2.2.6.2 最小池化

 <img src ="H:\gitfile\modiman.github.io\docs\_posts\imgs\最小.png"/>

#### 2.2.6.3 均值池化

<img src ="H:\gitfile\modiman.github.io\docs\_posts\imgs\均值.png"/>

### 2.2.7 通道

值得注意的是在卷积神经网络当中，卷积层的卷积核个数通常大于1，同层当中卷积核的个数也称为通道数，通常情况下卷积神经网络使用一组卷积核处理一个通道的数据矩阵。

### 2.2.8 全连接层 

### 2.2.9 全卷积

但是显然，直接使用全连接层将特征矩阵展开会破坏空间结构信息，为解决这一问题，人们开始考虑使用卷积层来代替全连接层，从而使得整个网络中不包含全连接层。

使用卷积层来代替全连接层的具体操作如下：假设全连接层的前一层输出为h×w大小的d张特征图，则可使用包含d个h×w大小卷积核所组成的卷积层代替该全连接层；假设全连接层的前一层输出为n维向量，则可使用n个1×1大小卷积核所组成的卷积层代替该全连接层。若卷积神经网络中不包含全连接层，这称之为**全卷积神经网络**。



## 2.3 循环神经网络

卷积神经网络和神经网络的神经元之间没有关联

对于机器翻译这类需要依赖上下文信息的任务存在弊端

而循环神经网络能解决类似的问题，循环神经网路对之前发生在文本数据序列中的单词是有一定记忆的，有助于系统获取上下文。

循环神经网络不仅将当前的输入数据作为网络输入，还将它们之前时刻输出数据一并作为输入。因此，可以尝试构建一个简单的模型结构，如图所示。为便于理解，它有一个输入层、一个具备特定激活函数的隐藏层，以及最终的输出层。

### 2.3.1 

